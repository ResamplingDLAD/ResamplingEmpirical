model_name: cnn
dataset_name: bgl
device: cpu
data_dir: ../dataset/bgl/
output_dir: results/bgl/100-0.25-SMOTE/
folder: bgl/
log_file: BGL.log
sample_size: None
sample_log_file: None
parser_type: None
log_format: None
regex: []
keep_para: False
st: 0.3
depth: 3
max_child: 100
tau: 0.5
is_process: True
is_instance: False
train_file: train_fixed100_instances.pkl
test_file: test_fixed100_instances.pkl
window_type: sliding
session_level: entry
window_size: 100.0
step_size: 100.0
train_size: 0.8
train_ratio: 1.0
valid_ratio: 0.1
test_ratio: 1.0
sampling_ratio: 0.25
sampling_method: SMOTE
max_epoch: 20
n_epochs_stop: 10
n_warm_up_epoch: 0
batch_size: 64
lr: 0.001
is_logkey: True
random_sample: False
is_time: False
min_freq: 1
seq_len: 10
min_len: 10
max_len: 512
mask_ratio: 0.5
adaptive_window: False
deepsvdd_loss: False
deepsvdd_loss_test: False
scale: None
hidden: 256
layers: 4
attn_heads: 4
num_workers: 5
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.0
sample: sliding_window
history_size: 10
embeddings: embeddings.json
sequentials: True
quantitatives: True
semantics: True
parameters: False
input_size: 300
hidden_size: 128
num_layers: 2
embedding_dim: 300
accumulation_step: 5
optimizer: adam
lr_decay_ratio: 0.1
num_candidates: 150
log_freq: 100
resume_path: False
num_encoder_layers: 1
num_decoder_layers: 1
dim_model: 300
num_heads: 8
dim_feedforward: 2048
transformers_dropout: 0.1
model_dir: results/bgl/100-0.25-SMOTE/cnn/
train_vocab: results/bgl/100-0.25-SMOTE/train.pkl
vocab_path: results/bgl/100-0.25-SMOTE/cnn_vocab.pkl
model_path: results/bgl/100-0.25-SMOTE/cnn/cnn.pth
scale_path: results/bgl/100-0.25-SMOTE/cnn/scale.pkl
